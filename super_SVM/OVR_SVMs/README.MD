## 1）一对多法（one-versus-rest,简称OVR SVMs）
### 目录写反了 将就着看一下....
　　
 训练时依次把某个类别的样本归为一类,其他剩余的样本归为另一类，这样k个类别的样本就构造出了k个SVM。分类时将未知样本分类为具有最大分类函数值的那类。

　　假如我有四类要划分（也就是4个Label），他们是A、B、C、D。

　　于是我在抽取训练集的时候，分别抽取

　　（1）A所对应的向量作为正集，B，C，D所对应的向量作为负集；

　　（2）B所对应的向量作为正集，A，C，D所对应的向量作为负集；

　　（3）C所对应的向量作为正集，A，B，D所对应的向量作为负集；

　　（4）D所对应的向量作为正集，A，B，C所对应的向量作为负集；

　　使用这四个训练集分别进行训练，然后的得到四个训练结果文件。

　　在测试的时候，把对应的测试向量分别利用这四个训练结果文件进行测试。

　　最后每个测试都有一个结果f1(x),f2(x),f3(x),f4(x)。

　　于是最终的结果便是这四个值中最大的一个作为分类结果。

评价

优点：训练k个分类器，个数较少，其分类速度相对较快。

 缺点：

  ①每个分类器的训练都是将全部的样本作为训练样本，这样在求解二次规划问题时，训练速度会随着训练样本的数量的增加而急剧减慢；

  ②同时由于负类样本的数据要远远大于正类样本的数据，从而出现了样本不对称的情况，且这种情况随着训练数据的增加而趋向严重。解决不对称的问题可以引入不同的惩罚因子，对样本点来说较少的正类采用较大的惩罚因子C；

  ③还有就是当有新的类别加进来时，需要对所有的模型进行重新训练。